{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, max as col_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bookmark(stage: str) -> str:\n",
    "    bookmark_path = f\"{hdfs_url}/data/{stage}/bookmark/\"\n",
    "    try:\n",
    "        df = spark.read.json(bookmark_path)\n",
    "        json_str = df.toJSON().first()\n",
    "        return json.loads(json_str)[\"lastUpdate\"]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def set_bookmark(stage: str, last_update: str) -> str:\n",
    "    bookmark_path = f\"{hdfs_url}/data/{stage}/bookmark/\"\n",
    "    json_str = json.dumps({\"lastUpdate\": last_update})\n",
    "    json_df = spark.read.json(spark.sparkContext.parallelize([json_str]))\n",
    "    json_df.write.mode(\"overwrite\").json(bookmark_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Ingestion\")\n",
    "    .config(\"spark.jars\", \"/spark/jars/postgresql-jdbc.jar\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:postgresql://postgres:5432/retail\"\n",
    "hdfs_url = \"hdfs://hadoop-namenode:8020\"\n",
    "\n",
    "db_username = os.environ.get(\"DB_USERNAME\", \"sparkuser\")\n",
    "db_password = os.environ.get(\"DB_PASSWORD\", \"sparkpassword\")\n",
    "\n",
    "connection_params = {\n",
    "    \"user\": db_username,\n",
    "    \"password\": db_password,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2025-05-25 09:52:44'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookmark = get_bookmark(\"bronze\")\n",
    "bookmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders table\n",
    "if bookmark:\n",
    "    orders_query = f\"(SELECT * FROM orders WHERE ordertime > '{bookmark}') AS orders\"\n",
    "else:\n",
    "    orders_query = \"orders\"\n",
    "\n",
    "# ordershistory table\n",
    "if bookmark:\n",
    "    orders_hist_query = f\"(SELECT * FROM ordershistory WHERE updatedat > '{bookmark}') AS ordershistory\"\n",
    "else:\n",
    "    orders_hist_query = \"ordershistory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# orders table\n",
    "(\n",
    "    spark.read.jdbc(url=jdbc_url, table=orders_query, properties=connection_params)\n",
    "    .withColumn(\"update\", to_date(col(\"ordertime\")))\n",
    "    .write.mode(\"append\")\n",
    "    .partitionBy(\"update\")\n",
    "    .parquet(\"hdfs://hadoop-namenode:8020/data/bronze/orders\")\n",
    ")\n",
    "\n",
    "# ordershistory table\n",
    "ordershistory_df = spark.read.jdbc(url=jdbc_url, table=orders_hist_query, properties=connection_params)\n",
    "\n",
    "(\n",
    "    ordershistory_df\n",
    "    .withColumn(\"update\", to_date(col(\"updatedat\")))\n",
    "    .write.mode(\"append\")\n",
    "    .partitionBy(\"update\")\n",
    "    .parquet(\"hdfs://hadoop-namenode:8020/data/bronze/ordershistory\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12511"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordershistory_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-09 13:37:44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "last_update = ordershistory_df.select(col_max(\"updatedat\")).first()[0]\n",
    "if last_update:\n",
    "    last_update = last_update.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(last_update)\n",
    "    set_bookmark(\"bronze\", last_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation (Silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Transformation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_url = \"hdfs://hadoop-namenode:8020/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f\"{hdfs_url}/bronze/orders\").createOrReplaceTempView(\"orders\")\n",
    "spark.read.parquet(f\"{hdfs_url}/bronze/ordershistory\").createOrReplaceTempView(\"ordershistory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT\n",
    "            o.orderid,\n",
    "            ordertime,\n",
    "            branch,\n",
    "            historyid,\n",
    "            status,\n",
    "            updatedat,\n",
    "            oh.update\n",
    "        FROM orders AS o\n",
    "        INNER JOIN ordershistory AS oh\n",
    "            ON o.orderid = oh.orderid\n",
    "        WHERE\n",
    "            status != \"INTRANSIT\"\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    joined_df.write.mode(\"overwrite\")\n",
    "    .partitionBy(\"update\")\n",
    "    .parquet(f\"{hdfs_url}/silver/allorders\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/16 17:27:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Serving\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_url = \"hdfs://hadoop-namenode:8020/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/16 17:27:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/06/16 17:28:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f\"{hdfs_url}/silver/allorders\").createOrReplaceTempView(\"allorders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        t1.orderid,\n",
    "        t1.historyid,\n",
    "        t1.branch,\n",
    "        t1.ordertime AS start,\n",
    "        t2.updatedat AS end,\n",
    "        (unix_timestamp(end) - unix_timestamp(start)) / 3600 AS deliverytime\n",
    "    FROM (SELECT * FROM allorders WHERE status = 'NEW') AS t1\n",
    "    INNER JOIN (SELECT * FROM allorders WHERE status = 'DELIVERED') AS t2\n",
    "    ON t1.orderid = t2.orderid\n",
    "    \"\"\"\n",
    ").createOrReplaceTempView(\"allorders_pivot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:====================================>                    (12 + 6) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|              branch|  avg_deliverytime|\n",
      "+--------------------+------------------+\n",
      "|    Harbor View Mall| 2343.730908532842|\n",
      "|    Sunset Park Mall| 2354.078234086242|\n",
      "|    Riverfront Plaza| 2440.241782637509|\n",
      "|Springfield Town ...| 2446.142755035738|\n",
      "|      Oakwood Square|2461.1467191601046|\n",
      "| Grand Avenue Center| 2512.957992688601|\n",
      "|         Metro Plaza| 2552.348954183267|\n",
      "|Pinecrest Shoppin...|2565.4971057884236|\n",
      "|       Lakeside Mall|  2579.23463648834|\n",
      "|   City Central Mall|2637.5592878592875|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "davg_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT branch, AVG(deliverytime) AS avg_deliverytime\n",
    "    FROM allorders_pivot\n",
    "    GROUP BY branch\n",
    "    ORDER BY 2 ASC\n",
    "    \"\"\"\n",
    ")\n",
    "davg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "davg_df.write.mode(\"overwrite\").csv(f\"{hdfs_url}/gold/deliveriesavg\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===================================>                    (12 + 6) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|              branch|deliveries_count|\n",
      "+--------------------+----------------+\n",
      "|   City Central Mall|            1554|\n",
      "|         Metro Plaza|            1004|\n",
      "| Grand Avenue Center|            1003|\n",
      "|    Harbor View Mall|             543|\n",
      "|Springfield Town ...|             513|\n",
      "|      Oakwood Square|             508|\n",
      "|    Riverfront Plaza|             503|\n",
      "|Pinecrest Shoppin...|             501|\n",
      "|    Sunset Park Mall|             487|\n",
      "|       Lakeside Mall|             486|\n",
      "+--------------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dcnt_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT branch, COUNT(*) AS deliveries_count\n",
    "    FROM allorders_pivot\n",
    "    GROUP BY branch\n",
    "    ORDER BY 2 DESC\n",
    "    \"\"\"\n",
    ")\n",
    "dcnt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dcnt_df.write.mode(\"overwrite\").csv(f\"{hdfs_url}/gold/deliveriescount\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
