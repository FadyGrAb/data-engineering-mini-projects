name: medallion
services:
  spark-master:
    image: spark:homemade
    container_name: spark-master
    restart: always
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MODE=master
    env_file:
      - ./spark/.env
    ports:
      - 8080:8080 # Master node ui
      - 4040:4040 # Driver ui
      - 18080:18080 # History server
      - 8888:8888 # Jupyter server
      - 6066:6066 # Spark http server
    volumes:
      - ./spark/jupyter/.bashrc:/root/.bashrc
      - ./spark/jupyter/jupyter_notebook_config.py:/root/.jupyter/jupyter_notebook_config.py
      - ./spark/spark-defaults.conf:/spark/conf/spark-defaults.conf
    entrypoint: ["/bin/bash", "-c", "/spark/run-spark.sh"]

  spark-worker-1:
    image: spark:homemade
    restart: always
    container_name: spark-worker-1
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MODE=worker
      - WORKER_MEMORY=2g
      - WORKER_CORES=6
    depends_on:
      - spark-master
    ports:
      - 8081:8081
    volumes:
      - ./spark/spark-defaults.conf:/spark/conf/spark-defaults.conf
    entrypoint: ["/bin/bash", "-c", "/spark/run-spark.sh"]
    
    
  spark-worker-2:
    image: spark:homemade
    build:
      context: ./spark
      dockerfile: Dockerfile
    restart: always
    container_name: spark-worker-2
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MODE=worker
      - WORKER_MEMORY=2g
      - WORKER_CORES=6
    depends_on:
      - spark-master
    ports:
      - 8082:8081
    volumes:
      - ./spark/spark-defaults.conf:/spark/conf/spark-defaults.conf
    entrypoint: ["/bin/bash", "-c", "/spark/run-spark.sh"]
  
  
  hadoop-namenode:
    image: apache/hadoop:3
    container_name: hadoop-namenode
    hostname: hadoop-namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870:9870 # Namenode UI
      - 8020:8020 # Hadoop IPC port
      - 50070:50070 #webhdfs
    env_file:
      - ./hadoop/config
    environment:
        ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    volumes:
      - ./spark/pipeline/:/opt/hadoop/pipeline
      - ./hadoop/init.sh:/opt/hadoop/init.sh
  hadoop-datanode:
    container_name: hadoop-datanode
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    environment:
      - 50075:50075 #webhdfs
    env_file:
      - ./hadoop/config    
  hadoop-resourcemanager:
    image: apache/hadoop:3
    container_name: hadoop-resourcemanager
    hostname: hadoop-resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
        - 8088:8088
    env_file:
      - ./hadoop/config
  hadoop-nodemanager:
    image: apache/hadoop:3
    container_name: hadoop-nodemanager
    command: ["yarn", "nodemanager"]
    env_file:
      - ./hadoop/config

  

  dashboard:
    image: dashboard:homemade
    container_name: dashboard
    # environment:
    #   - FLASK_ENV=development
    #   - FLASK_DEBUG=1
    ports:
      - 5000:5000
    volumes:
      - ./dashboard/app.py:/app/app.py