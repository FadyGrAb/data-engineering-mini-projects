{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"Ingestion\") \\\n",
    "            .config(\"spark.jars\", \"/spark/jars/postgresql-jdbc.jar\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_username = os.environ.get(\"DB_USERNAME\")\n",
    "db_password = os.environ.get(\"DB_PASSWORD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:postgresql://postgres:5432/retail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_params = {\n",
    "    \"user\": db_username,\n",
    "    \"password\": db_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"orders\",\n",
    "    properties=connection_params\n",
    ").createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_history_df = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"ordershistory\",\n",
    "    properties=connection_params\n",
    ").createOrReplaceTempView(\"ordershistory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        o.orderid,\n",
    "        historyid,\n",
    "        ordertime,\n",
    "        branch,\n",
    "        status,\n",
    "        updatedat\n",
    "    FROM orders AS o\n",
    "    INNER JOIN ordershistory AS oh\n",
    "    ON o.orderid = oh.orderid\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.withColumn(\"update\", to_date(col(\"updatedat\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.write.mode(\"overwrite\").partitionBy(\"update\").parquet(\"hdfs://hadoop:pass@hadoop-namenode:8020/data/bronze/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.createOrReplaceTempView(\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"SELECT * \n",
    "        FROM result\n",
    "        ORDER BY historyid\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col\n",
    "import os\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Ingestion\")\n",
    "    .config(\"spark.jars\", \"/spark/jars/postgresql-jdbc.jar\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "db_username = os.environ.get(\"DB_USERNAME\")\n",
    "db_password = os.environ.get(\"DB_PASSWORD\")\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/retail\"\n",
    "\n",
    "connection_params = {\n",
    "    \"user\": db_username,\n",
    "    \"password\": db_password,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "\n",
    "orders_df = (\n",
    "    spark.read.jdbc(url=jdbc_url, table=\"orders\", properties=connection_params)\n",
    "    .withColumn(\"update\", to_date(col(\"ordertime\")))\n",
    "    .write.mode(\"overwrite\")\n",
    "    .partitionBy(\"update\")\n",
    "    .parquet(\"hdfs://hadoop:pass@hadoop-namenode:8020/data/bronze/orders\")\n",
    ")\n",
    "\n",
    "orders_history_df = (\n",
    "    spark.read.jdbc(url=jdbc_url, table=\"ordershistory\", properties=connection_params)\n",
    "    .withColumn(\"update\", to_date(col(\"updatedat\")))\n",
    "    .write.mode(\"overwrite\")\n",
    "    .partitionBy(\"update\")\n",
    "    .parquet(\"hdfs://hadoop:pass@hadoop-namenode:8020/data/bronze/ordershistory\")\n",
    ")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
